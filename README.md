**TASK-1**
The goal of this task is to implement a Retrieval-Augmented Generation (RAG) pipeline for interacting with multiple PDF files that contain semi-structured data. This system aims to facilitate efficient information retrieval and response generation by processing the PDFs in several key stages. First, it ingests the data from PDF files, extracting and segmenting the text into logical chunks. These chunks are then converted into vector embeddings using a pre-trained model and stored in a vector database for efficient retrieval. When a user asks a query, the system converts the question into vector embeddings and searches the database to retrieve the most relevant chunks. These chunks, along with the user query, are passed to a large language model (LLM) to generate an appropriate response. If the query involves comparisons, the system identifies relevant data points across multiple PDFs, retrieves the corresponding chunks, processes the data, and presents it in a structured format (such as a table or bullet points). The response generation process ensures factual accuracy by incorporating retrieved data directly into the response, ensuring that the answer is both relevant and grounded in the data extracted from the PDFs.

In this context, the example PDF data would be processed by first extracting specific data, such as unemployment rates based on degree type from page 2, and then retrieving and displaying tabular data from page 6 for user queries. This would provide a comprehensive and efficient way for users to interact with semi-structured data in PDF form.

**TASK2**
The task described involves building a Retrieval-Augmented Generation (RAG) pipeline that enables users to interact with data derived from websites. The first key step is data ingestion, where the system takes in a list of URLs or website addresses provided by the user. The pipeline then crawls and scrapes the websites, extracting important information such as key data fields, metadata, and textual content. To ensure that this data is usable for future queries, the content is broken into manageable chunks, which are then converted into vector embeddings using a pre-trained model. These embeddings are stored in a vector database alongside relevant metadata, allowing for efficient retrieval when users need information. By embedding the data, the system is able to represent it in a form that can be quickly compared to user queries.

When a user submits a question, the query is converted into vector embeddings using the same model that was used for the website content. The system then performs a similarity search in the vector database to find the most relevant pieces of content that align with the userâ€™s query. The relevant chunks are retrieved and passed to a large language model (LLM) along with a contextual prompt that guides the generation of a detailed, accurate response. This allows the system to generate answers based on real-time information sourced from the websites, ensuring that the responses are factually correct and contextually relevant. By combining the power of retrieval and generation, the RAG pipeline effectively enhances the quality of the user experience, providing tailored, precise answers directly from the scraped content.
